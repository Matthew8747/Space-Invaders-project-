{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ffaa811",
   "metadata": {},
   "source": [
    "# Space Invaders: PPO + Rainbow \n",
    "\n",
    "This notebook contains two independent agents for Atari Space Invaders:\n",
    "\n",
    "- **PPO (on-policy)**: vectorized rollouts + GAE + clipped surrogate objective  \n",
    "- **Rainbow DQN (off-policy)**: NoisyNet + Dueling + C51 + PER + n-step returns\n",
    "\n",
    "They share environment construction utilities and evaluation helpers, but training loops remain separate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Runtime / Device ----\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation, ResizeObservation, GrayscaleObservation\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "print(\"torch.cuda.is_available():\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"DEVICE:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896c33b0",
   "metadata": {},
   "source": [
    "## Shared Atari environment utilities\n",
    " We standardise environment construction for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c88ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_atari_env(\n",
    "    seed: int = 0,\n",
    "    *,\n",
    "    episodic_life: bool = False,\n",
    "    sticky_actions: bool = True,\n",
    "    frame_skip: int = 4,\n",
    "    frame_stack: int = 4,\n",
    "    render_mode=None,\n",
    "):\n",
    "    \"\"\"Create ALE/SpaceInvaders-v5 with standard preprocessing.\n",
    "\n",
    "    - frameskip at ALE level set to 1 (we manage skipping in AtariPreprocessing)\n",
    "    - sticky actions optional via repeat_action_probability\n",
    "    - AtariPreprocessing handles resize, grayscale, (optional) episodic life, noop reset\n",
    "    - FrameStackObservation stacks 4 frames -> (4,84,84)\n",
    "    \"\"\"\n",
    "    env = gym.make(\n",
    "        \"ALE/SpaceInvaders-v5\",\n",
    "        frameskip=1,\n",
    "        repeat_action_probability=0.25 if sticky_actions else 0.0,\n",
    "        full_action_space=False,\n",
    "        render_mode=render_mode,\n",
    "    )\n",
    "\n",
    "    env = AtariPreprocessing(\n",
    "        env,\n",
    "        noop_max=30,\n",
    "        frame_skip=frame_skip,\n",
    "        screen_size=84,\n",
    "        terminal_on_life_loss=episodic_life,\n",
    "        grayscale_obs=True,\n",
    "        grayscale_newaxis=False,\n",
    "        scale_obs=False,\n",
    "    )\n",
    "\n",
    "    env = FrameStackObservation(env, frame_stack)\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "def fix_obs(obs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Ensure observation is in (4,84,84) uint8 format.\n",
    "\n",
    "    Handles possible formats returned by wrappers / vector envs.\n",
    "    \"\"\"\n",
    "    # FrameStackObservation commonly returns (4,84,84) already.\n",
    "    if isinstance(obs, np.ndarray):\n",
    "        if obs.shape == (84, 84, 4):\n",
    "            return np.transpose(obs, (2, 0, 1))\n",
    "        \n",
    "        if obs.shape == (4, 84, 84):\n",
    "            return obs\n",
    "        \n",
    "        if obs.shape == (84, 84):\n",
    "            return np.repeat(obs[None, ...], 4, axis=0)\n",
    "        \n",
    "    return np.array(obs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fb145",
   "metadata": {},
   "source": [
    "## PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO model\n",
    "class PPOCNNPolicy(nn.Module):\n",
    "    def __init__(self, action_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1), nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.policy = nn.Linear(512, action_dim)\n",
    "        self.value = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x expected float32 or uint8-like; normalise in-network.\n",
    "        x = x / 255.0\n",
    "        x = self.conv(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return self.policy(x), self.value(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b922f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PPO: Training / Evaluation ----\n",
    "def train_ppo(config: dict):\n",
    "    \"\"\"Train PPO on Space Invaders.\n",
    "\n",
    "    Notes:\n",
    "    - PPO is on-policy: do not reuse experience across updates.\n",
    "    - Clip rewards to {-1,0,1} for stability but track raw episode score separately.\n",
    "    \"\"\"\n",
    "    num_envs = int(config.get(\"num_envs\", 8))\n",
    "    num_steps = int(config.get(\"num_steps\", 512))\n",
    "    total_steps = int(config.get(\"total_steps\", 10_000_000))\n",
    "\n",
    "    learning_rate = float(config.get(\"learning_rate\", 2.5e-4))\n",
    "    gamma = float(config.get(\"gamma\", 0.99))\n",
    "\n",
    "    gae_lambda = float(config.get(\"gae_lambda\", 0.95))\n",
    "    clip_epsilon = float(config.get(\"clip_epsilon\", 0.1))\n",
    "\n",
    "    epochs = int(config.get(\"epochs\", 4))\n",
    "    minibatch_size = int(config.get(\"minibatch_size\", 512))\n",
    "\n",
    "    entropy_coef = float(config.get(\"entropy_coef\", 0.01))\n",
    "    value_coef = float(config.get(\"value_coef\", 0.5))\n",
    "    linear_lr_decay = bool(config.get(\"linear_lr_decay\", True))\n",
    "\n",
    "    checkpoint_path = str(config.get(\"checkpoint_path\", \"space_invaders_final_ppo.pth\"))\n",
    "    save_every = int(config.get(\"save_every\", 1_000_000))\n",
    "    save_template = str(config.get(\"save_template\", \"space_invaders_ppo_{}M.pth\"))\n",
    "    load_path = config.get(\"load_path\", None)\n",
    "\n",
    "    # Vector env - use SyncVectorEnv for Windows compatibility\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [lambda i=i: make_atari_env(seed=i, episodic_life=False, sticky_actions=False) for i in range(num_envs)]\n",
    "    )\n",
    "    obs, _ = envs.reset()\n",
    "    action_dim = envs.single_action_space.n\n",
    "\n",
    "    model = PPOCNNPolicy(action_dim).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, eps=1e-5)\n",
    "\n",
    "    # Load\n",
    "    if load_path and os.path.exists(load_path):\n",
    "        print(f\"[PPO] Loading weights from {load_path}\")\n",
    "        model.load_state_dict(torch.load(load_path, map_location=DEVICE))\n",
    "\n",
    "    elif os.path.exists(checkpoint_path):\n",
    "        print(f\"[PPO] Loading weights from {checkpoint_path}\")\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
    "\n",
    "    else:\n",
    "        print(\"[PPO] No checkpoint found. Starting from scratch.\")\n",
    "\n",
    "    episode_scores = np.zeros(num_envs, dtype=np.float32)\n",
    "    recent_scores = deque(maxlen=100)\n",
    "\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    while global_step < total_steps:\n",
    "        if linear_lr_decay:\n",
    "            frac = 1.0 - (global_step / max(1, total_steps))\n",
    "            optimizer.param_groups[0][\"lr\"] = learning_rate * frac\n",
    "\n",
    "        obs_buf, act_buf, logp_buf, rew_buf, done_buf, val_buf = [], [], [], [], [], []\n",
    "\n",
    "        for _ in range(num_steps):\n",
    "            global_step += num_envs\n",
    "\n",
    "            with torch.no_grad():\n",
    "                obs_t = torch.tensor(obs, dtype=torch.float32, device=DEVICE)\n",
    "                logits, values = model(obs_t)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                actions = dist.sample()\n",
    "\n",
    "            next_obs, rewards, terminations, truncations, _ = envs.step(actions.cpu().numpy())\n",
    "            dones = terminations | truncations\n",
    "\n",
    "            # Raw score tracking\n",
    "            episode_scores += rewards\n",
    "            for i, done in enumerate(dones):\n",
    "                if done:\n",
    "                    recent_scores.append(float(episode_scores[i]))\n",
    "                    episode_scores[i] = 0.0\n",
    "\n",
    "            clipped_rewards = np.sign(rewards)\n",
    "\n",
    "            obs_buf.append(obs)\n",
    "            act_buf.append(actions.cpu().numpy())\n",
    "            logp_buf.append(dist.log_prob(actions).detach().cpu().numpy())\n",
    "            rew_buf.append(clipped_rewards)\n",
    "            done_buf.append(dones)\n",
    "            val_buf.append(values.squeeze().detach().cpu().numpy())\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "            if global_step % save_every < num_envs:\n",
    "                m = int(global_step / 1_000_000)\n",
    "                save_name = save_template.format(m)\n",
    "                torch.save(model.state_dict(), save_name)\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"[PPO] Saved: {save_name}\")\n",
    "\n",
    "        # GAE\n",
    "        obs_arr = torch.tensor(np.array(obs_buf), dtype=torch.float32, device=DEVICE)\n",
    "        act_arr = torch.tensor(np.array(act_buf), device=DEVICE)\n",
    "        logp_arr = torch.tensor(np.array(logp_buf), dtype=torch.float32, device=DEVICE)\n",
    "        rew_arr = np.array(rew_buf)\n",
    "        done_arr = np.array(done_buf)\n",
    "        val_arr = np.array(val_buf)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_val = model(torch.tensor(obs, dtype=torch.float32, device=DEVICE))[1].squeeze().cpu().numpy()\n",
    "\n",
    "        adv = np.zeros_like(rew_arr)\n",
    "        lastgaelam = 0.0\n",
    "\n",
    "        for t in reversed(range(num_steps)):\n",
    "            next_nonterminal = 1.0 - done_arr[t].astype(np.float32)\n",
    "            next_values = next_val if t == num_steps - 1 else val_arr[t + 1]\n",
    "            delta = rew_arr[t] + gamma * next_values * next_nonterminal - val_arr[t]\n",
    "            lastgaelam = delta + gamma * gae_lambda * next_nonterminal * lastgaelam\n",
    "            adv[t] = lastgaelam\n",
    "\n",
    "        returns = adv + val_arr\n",
    "        adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "        b_obs = obs_arr.reshape(-1, 4, 84, 84)\n",
    "        b_act = act_arr.reshape(-1)\n",
    "        b_logp = logp_arr.reshape(-1)\n",
    "        b_adv = torch.tensor(adv.reshape(-1), dtype=torch.float32, device=DEVICE)\n",
    "        b_ret = torch.tensor(returns.reshape(-1), dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        batch_size = num_envs * num_steps\n",
    "        idxs = np.arange(batch_size)\n",
    "\n",
    "        losses = []\n",
    "        for _ in range(epochs):\n",
    "            np.random.shuffle(idxs)\n",
    "\n",
    "            for start in range(0, batch_size, minibatch_size):\n",
    "                mb_idx = idxs[start:start + minibatch_size]\n",
    "\n",
    "                logits, values = model(b_obs[mb_idx])\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                new_logp = dist.log_prob(b_act[mb_idx])\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                ratio = (new_logp - b_logp[mb_idx]).exp()\n",
    "                pg_loss = -torch.min(\n",
    "                    ratio * b_adv[mb_idx],\n",
    "                    torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * b_adv[mb_idx],\n",
    "                ).mean()\n",
    "                v_loss = ((values.squeeze() - b_ret[mb_idx]) ** 2).mean()\n",
    "                loss = pg_loss + value_coef * v_loss - entropy_coef * entropy\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(float(loss.item()))\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        fps = int(global_step / max(1e-8, elapsed))\n",
    "        avg_score = float(np.mean(recent_scores)) if recent_scores else 0.0\n",
    "        lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        print(f\"[PPO] Step {global_step}/{total_steps} | AvgScore(100): {avg_score:.0f} | FPS: {fps} | LR: {lr_now:.2e}\")\n",
    "\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    envs.close()\n",
    "    \n",
    "    print(f\"[PPO] Training complete. Saved to {checkpoint_path}\")\n",
    "    return checkpoint_path\n",
    "\n",
    "\n",
    "def get_noop_action(env) -> int:\n",
    "    \"\"\"Get the NOOP action index for the environment.\"\"\"\n",
    "    try:\n",
    "        meanings = env.unwrapped.get_action_meanings()\n",
    "        if \"NOOP\" in meanings:\n",
    "            return meanings.index(\"NOOP\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return 0\n",
    "\n",
    "\n",
    "def print_eval_summary(agent_name: str, returns: list):\n",
    "    \"\"\"Print comprehensive evaluation summary metrics.\"\"\"\n",
    "    scores = np.array(returns)\n",
    "    n = len(scores)\n",
    "    \n",
    "    mean_score = float(np.mean(scores))\n",
    "    median_score = float(np.median(scores))\n",
    "    std_score = float(np.std(scores))\n",
    "    min_score = float(np.min(scores))\n",
    "    max_score = float(np.max(scores))\n",
    "    \n",
    "    q25 = float(np.percentile(scores, 25))\n",
    "    q75 = float(np.percentile(scores, 75))\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    coef_var = std_score / mean_score if mean_score != 0 else 0.0\n",
    "    std_err = std_score / np.sqrt(n) if n > 0 else 0.0\n",
    "    \n",
    "    # 95% CI for mean\n",
    "    ci_low = mean_score - 1.96 * std_err\n",
    "    ci_high = mean_score + 1.96 * std_err\n",
    "    \n",
    "    best_run = int(np.argmax(scores)) + 1\n",
    "    worst_run = int(np.argmin(scores)) + 1\n",
    "    \n",
    "    # Success rates\n",
    "    rate_200 = 100.0 * np.sum(scores >= 200) / n\n",
    "    rate_500 = 100.0 * np.sum(scores >= 500) / n\n",
    "    rate_1000 = 100.0 * np.sum(scores >= 1000) / n\n",
    "    rate_1500 = 100.0 * np.sum(scores >= 1500) / n\n",
    "    \n",
    "    print(f\"========== {agent_name} SUMMARY METRICS ==========\")\n",
    "    print(f\"{'Runs:':<22}{n}\")\n",
    "    print(f\"{'Mean score:':<22}{mean_score:.2f}\")\n",
    "    print(f\"{'Median score:':<22}{median_score:.2f}\")\n",
    "    print(f\"{'Std deviation:':<22}{std_score:.2f}\")\n",
    "    print(f\"{'Min score:':<22}{min_score:.1f}\")\n",
    "    print(f\"{'Max score:':<22}{max_score:.1f}\")\n",
    "    print(f\"{'25th percentile:':<22}{q25:.2f}\")\n",
    "    print(f\"{'75th percentile:':<22}{q75:.2f}\")\n",
    "    print(f\"{'IQR:':<22}{iqr:.2f}\")\n",
    "    print(f\"{'Coeff. of variation:':<22}{coef_var:.3f}\")\n",
    "    print(f\"{'Standard error:':<22}{std_err:.2f}\")\n",
    "    print(f\"{'95% CI (mean):':<22}[{ci_low:.2f}, {ci_high:.2f}]\")\n",
    "    print(f\"{'Best run #:':<22}{best_run}\")\n",
    "    print(f\"{'Worst run #:':<22}{worst_run}\")\n",
    "    print()\n",
    "    print(\"--- Success Rates ---\")\n",
    "    print(f\"Score >= 200: {rate_200:.2f}%\")\n",
    "    print(f\"Score >= 500: {rate_500:.2f}%\")\n",
    "    print(f\"Score >= 1000: {rate_1000:.2f}%\")\n",
    "    print(f\"Score >= 1500: {rate_1500:.2f}%\")\n",
    "    print(\"=\" * (len(agent_name) + 36))\n",
    "\n",
    "\n",
    "def make_ppo_eval_env(sticky_action_prob: float = 0.25):\n",
    "    \"\"\"Create evaluation environment for PPO with sticky keys.\n",
    "    \n",
    "    Uses simpler wrappers matching the final_eval.py approach:\n",
    "    - frameskip=4 at ALE level\n",
    "    - ResizeObservation -> GrayscaleObservation -> FrameStackObservation\n",
    "    \"\"\"\n",
    "    env = gym.make(\n",
    "        \"ALE/SpaceInvaders-v5\",\n",
    "        frameskip=4,\n",
    "        repeat_action_probability=sticky_action_prob,\n",
    "        render_mode=None,\n",
    "    )\n",
    "    env = ResizeObservation(env, (84, 84))\n",
    "    env = GrayscaleObservation(env)\n",
    "    env = FrameStackObservation(env, stack_size=4)\n",
    "    return env\n",
    "\n",
    "\n",
    "def obs_to_tensor(obs, device) -> torch.Tensor:\n",
    "    \"\"\"Convert observation to tensor for model input.\"\"\"\n",
    "    obs_np = np.asarray(obs)\n",
    "    # Handle possible (4,84,84,1) shape from wrappers\n",
    "    if obs_np.ndim == 4 and obs_np.shape[-1] == 1:\n",
    "        obs_np = obs_np.squeeze(-1)\n",
    "    return torch.tensor(obs_np, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "\n",
    "def evaluate_ppo(\n",
    "    checkpoint_path: str,\n",
    "    episodes: int = 100,\n",
    "    seed: int = 123,\n",
    "    max_random_noops: int = 30,\n",
    "    sticky_action_prob: float = 0.25,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"Evaluate PPO agent with sticky keys and random no-op starts.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the model checkpoint.\n",
    "        episodes: Number of evaluation episodes.\n",
    "        seed: Random seed for reproducibility.\n",
    "        max_random_noops: Maximum number of random no-op actions at episode start.\n",
    "        sticky_action_prob: Probability of repeating previous action (sticky keys).\n",
    "        verbose: If True, print per-episode scores and summary statistics.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (mean_score, std_score).\n",
    "    \"\"\"\n",
    "    env = make_ppo_eval_env(sticky_action_prob=sticky_action_prob)\n",
    "    action_dim = env.action_space.n\n",
    "    noop_action = get_noop_action(env)\n",
    "    \n",
    "    model = PPOCNNPolicy(action_dim).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
    "    model.eval()\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    returns = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[PPO EVAL] Starting evaluation: {episodes} episodes, max_noops={max_random_noops}\")\n",
    "        print(f\"[PPO EVAL] Sticky Keys: {sticky_action_prob > 0} (Prob: {sticky_action_prob})\")\n",
    "        print(f\"[PPO EVAL] Checkpoint: {checkpoint_path}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        \n",
    "        # Random no-op actions at episode start\n",
    "        n_noops = int(rng.integers(0, max_random_noops + 1))\n",
    "        terminated = truncated = False\n",
    "        for _ in range(n_noops):\n",
    "            obs, _, terminated, truncated, _ = env.step(noop_action)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        total = 0.0\n",
    "        \n",
    "        while not done:\n",
    "            obs_t = obs_to_tensor(obs, DEVICE)\n",
    "            with torch.no_grad():\n",
    "                logits, _ = model(obs_t)\n",
    "                action = int(torch.argmax(logits, dim=1).item())\n",
    "            obs, r, terminated, truncated, _ = env.step(action)\n",
    "            done = bool(terminated or truncated)\n",
    "            total += float(r)\n",
    "        \n",
    "        returns.append(total)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[PPO EVAL] Episode {ep + 1}/{episodes} | Score: {total:.0f} | NoOps: {n_noops}\")\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    mean_score = float(np.mean(returns))\n",
    "    std_score = float(np.std(returns))\n",
    "    \n",
    "    if verbose:\n",
    "        print()\n",
    "        print_eval_summary(\"PPO\", returns)\n",
    "    \n",
    "    return mean_score, std_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece153be",
   "metadata": {},
   "source": [
    "## Rainbow DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Rainbow Hyperparams ----\n",
    "SAVE_DIR = \"./rainbow_space_invaders_log\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "NUM_ENVS_RB = 4\n",
    "FRAME_SKIP = 4\n",
    "FRAMES_PER_STEP = NUM_ENVS_RB * FRAME_SKIP\n",
    "\n",
    "TOTAL_FRAMES = 32_000_000\n",
    "TOTAL_STEPS = TOTAL_FRAMES // FRAMES_PER_STEP\n",
    "\n",
    "NOISY_SIGMA_INIT = 0.5\n",
    "N_STEP = 3\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "REPLAY_SIZE = 1_000_000\n",
    "\n",
    "PER_ALPHA = 0.5\n",
    "PER_BETA_START = 0.4\n",
    "PER_BETA_END = 1.0\n",
    "PER_BETA_ANNEAL_STEPS = max(1, TOTAL_STEPS)\n",
    "TARGET_UPDATE = 32_000 // FRAMES_PER_STEP\n",
    "\n",
    "LEARNING_RATE_RB = 6.25e-5\n",
    "ADAM_EPS = 1.5e-4\n",
    "\n",
    "NUM_ATOMS = 51\n",
    "V_MIN = -10\n",
    "V_MAX = 10\n",
    "\n",
    "LEARNING_START = 80_000 // FRAMES_PER_STEP\n",
    "PER_MAX_PRIORITY = 1.0\n",
    "\n",
    "csv_path = os.path.join(SAVE_DIR, \"training_log.csv\")\n",
    "if not os.path.exists(csv_path):\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\"step\", \"episode_reward\", \"avg_reward_100\",\n",
    "                                \"loss\", \"q_mean\", \"q_max\", \"eval_mean\", \"eval_std\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71884be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Noisy Linear ----\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, inp, outp):\n",
    "        super().__init__()\n",
    "        self.in_features = inp\n",
    "        self.out_features = outp\n",
    "        self.noisy = True\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.empty(outp, inp))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(outp, inp))\n",
    "        self.register_buffer(\"weight_epsilon_i\", torch.empty(inp))\n",
    "        self.register_buffer(\"weight_epsilon_j\", torch.empty(outp))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.empty(outp))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(outp))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.empty(outp))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / np.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        sigma_init = NOISY_SIGMA_INIT / np.sqrt(self.in_features)\n",
    "        self.weight_sigma.data.fill_(sigma_init)\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(sigma_init)\n",
    "\n",
    "    @staticmethod\n",
    "    def _f(x):\n",
    "        return x.sign() * x.abs().sqrt()\n",
    "\n",
    "    def reset_noise(self):\n",
    "        self.weight_epsilon_i.normal_()\n",
    "        self.weight_epsilon_j.normal_()\n",
    "        self.bias_epsilon.normal_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.noisy:\n",
    "            eps_i = self._f(self.weight_epsilon_i)\n",
    "            eps_j = self._f(self.weight_epsilon_j)\n",
    "            w_eps = eps_j.unsqueeze(1) * eps_i.unsqueeze(0)\n",
    "            w = self.weight_mu + self.weight_sigma * w_eps\n",
    "            b = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            w = self.weight_mu\n",
    "            b = self.bias_mu\n",
    "        return nn.functional.linear(x, w, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df139fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Rainbow Network (Dueling + Noisy + C51) ----\n",
    "class RainbowDQN(nn.Module):\n",
    "    def __init__(self, action_dim, num_atoms=NUM_ATOMS, v_min=V_MIN, v_max=V_MAX):\n",
    "        super().__init__()\n",
    "        self.num_atoms = num_atoms\n",
    "        self.action_dim = action_dim\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.register_buffer(\"support\", torch.linspace(v_min, v_max, num_atoms))\n",
    "\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 8, 4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, 1), nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc = nn.Sequential(NoisyLinear(64 * 7 * 7, 512), nn.ReLU())\n",
    "        self.value = NoisyLinear(512, num_atoms)\n",
    "        self.advantage = NoisyLinear(512, action_dim * num_atoms)\n",
    "\n",
    "    def set_noise(self, noisy: bool):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, NoisyLinear):\n",
    "                m.noisy = noisy\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, NoisyLinear):\n",
    "                m.reset_noise()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0\n",
    "        x = self.fc(self.feature(x))\n",
    "        v = self.value(x).view(-1, 1, self.num_atoms)\n",
    "        a = self.advantage(x).view(-1, self.action_dim, self.num_atoms)\n",
    "        q_atoms = v + a - a.mean(1, keepdim=True)\n",
    "        return torch.softmax(q_atoms, dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a50fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PER (SumTree + Prioritized Replay Buffer) ----\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = int(capacity)\n",
    "        self.tree = np.zeros(2 * self.capacity - 1, dtype=np.float32)\n",
    "        self.data = [None] * self.capacity\n",
    "        self.write = 0\n",
    "        self.n_entries = 0\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "        self.tree[parent] += change\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return float(self.tree[0])\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "        self.write = (self.write + 1) % self.capacity\n",
    "        self.n_entries = min(self.n_entries + 1, self.capacity)\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        data_idx = idx - self.capacity + 1\n",
    "        return idx, float(self.tree[idx]), self.data[data_idx]\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, size=REPLAY_SIZE, alpha=PER_ALPHA):\n",
    "        self.alpha = alpha\n",
    "        self.capacity = int(size)\n",
    "        self.tree = SumTree(self.capacity)\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def add(self, transition):\n",
    "        p = self.max_priority\n",
    "        self.tree.add((p ** self.alpha), transition)\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        batch, tree_idxs, priorities = [], [], []\n",
    "        total = self.tree.total()\n",
    "        if total <= 0 or self.tree.n_entries == 0:\n",
    "            return None, None, None\n",
    "\n",
    "        segment = total / batch_size\n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            tree_idx, p, data = self.tree.get(s)\n",
    "            if data is None:\n",
    "                return None, None, None\n",
    "            batch.append(data)\n",
    "            tree_idxs.append(tree_idx)\n",
    "            priorities.append(p)\n",
    "\n",
    "        probs = np.array(priorities, dtype=np.float32) / (total + 1e-8)\n",
    "        weights = (self.tree.n_entries * probs + 1e-8) ** (-beta)\n",
    "        weights = weights / (weights.max() + 1e-8)\n",
    "        weights = torch.tensor(weights, device=DEVICE, dtype=torch.float32)\n",
    "        return batch, tree_idxs, weights\n",
    "\n",
    "    def update(self, tree_idxs, priorities):\n",
    "        for tree_idx, p in zip(tree_idxs, priorities):\n",
    "            p = float(p)\n",
    "            p = min(max(p, 1e-6), PER_MAX_PRIORITY)\n",
    "            self.tree.update(int(tree_idx), (p ** self.alpha))\n",
    "            self.max_priority = max(self.max_priority, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc1014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- N-step buffer ----\n",
    "class NStepBuffer:\n",
    "    def __init__(self, n=N_STEP, gamma=GAMMA):\n",
    "        self.n = n\n",
    "        self.gamma = gamma\n",
    "        self.buf = deque()\n",
    "\n",
    "    def reset(self):\n",
    "        self.buf.clear()\n",
    "\n",
    "    def add(self, s, a, r, ns, done):\n",
    "        self.buf.append((s, a, r, ns, done))\n",
    "\n",
    "    def _compute_nstep(self, k):\n",
    "        R = 0.0\n",
    "        ns_k = None\n",
    "        done_k = False\n",
    "        for i in range(k):\n",
    "            _, _, r, ns, done = self.buf[i]\n",
    "            R += (self.gamma ** i) * float(r)\n",
    "            ns_k = ns\n",
    "            if done:\n",
    "                done_k = True\n",
    "                break\n",
    "        s0, a0 = self.buf[0][0], self.buf[0][1]\n",
    "        return s0, a0, R, ns_k, done_k\n",
    "\n",
    "    def pop_nstep_if_ready(self):\n",
    "        if len(self.buf) < self.n:\n",
    "            return None\n",
    "        out = self._compute_nstep(self.n)\n",
    "        self.buf.popleft()\n",
    "        return out\n",
    "\n",
    "    def flush_all(self):\n",
    "        outs = []\n",
    "        while len(self.buf) > 0:\n",
    "            k = min(self.n, len(self.buf))\n",
    "            outs.append(self._compute_nstep(k))\n",
    "            self.buf.popleft()\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e43e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- C51 Projection + Loss ----\n",
    "def c51_projection(next_dist, rewards, dones, support, v_min, v_max, num_atoms, gamma):\n",
    "    delta_z = (v_max - v_min) / (num_atoms - 1)\n",
    "    Tz = rewards[:, None] + (1.0 - dones[:, None]) * (gamma * support[None, :])\n",
    "    Tz = Tz.clamp(v_min, v_max)\n",
    "    b = (Tz - v_min) / delta_z\n",
    "    l = b.floor().long()\n",
    "    u = b.ceil().long()\n",
    "\n",
    "    proj = torch.zeros_like(next_dist)\n",
    "    eq = (u == l)\n",
    "    proj.scatter_add_(1, l.clamp(0, num_atoms - 1), next_dist * eq.float())\n",
    "\n",
    "    ne = ~eq\n",
    "    if ne.any():\n",
    "        idx = ne.nonzero(as_tuple=True)[0]\n",
    "        l_ne = l[ne].clamp(0, num_atoms - 1)\n",
    "        u_ne = u[ne].clamp(0, num_atoms - 1)\n",
    "        b_ne = b[ne]\n",
    "        d_ne = next_dist[ne]\n",
    "\n",
    "        proj.index_put_((idx, l_ne), d_ne * (u_ne.float() - b_ne), accumulate=True)\n",
    "        proj.index_put_((idx, u_ne), d_ne * (b_ne - l_ne.float()), accumulate=True)\n",
    "\n",
    "    proj = proj / (proj.sum(dim=1, keepdim=True) + 1e-8)\n",
    "    return proj\n",
    "\n",
    "\n",
    "def rainbow_loss(model, target, batch, weights, n_step=N_STEP, gamma=GAMMA):\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    states = torch.from_numpy(np.stack(states)).to(DEVICE).float()\n",
    "    next_states = torch.from_numpy(np.stack(next_states)).to(DEVICE).float()\n",
    "    actions = torch.tensor(actions, device=DEVICE, dtype=torch.long)\n",
    "    rewards = torch.tensor(rewards, device=DEVICE, dtype=torch.float32)\n",
    "    dones = torch.tensor(dones, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    gamma_n = gamma ** n_step\n",
    "\n",
    "    model.reset_noise()\n",
    "    target.reset_noise()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_dist_all = model(next_states)\n",
    "        q_next = (next_dist_all * model.support).sum(2)\n",
    "        next_actions = q_next.argmax(1)\n",
    "\n",
    "        target_dist_all = target(next_states)\n",
    "        next_dist = target_dist_all[torch.arange(len(batch), device=DEVICE), next_actions]\n",
    "\n",
    "        proj = c51_projection(\n",
    "            next_dist=next_dist,\n",
    "            rewards=rewards,\n",
    "            dones=dones,\n",
    "            support=model.support,\n",
    "            v_min=model.v_min,\n",
    "            v_max=model.v_max,\n",
    "            num_atoms=model.num_atoms,\n",
    "            gamma=gamma_n,\n",
    "        )\n",
    "\n",
    "    dist_all = model(states)\n",
    "    dist = dist_all[torch.arange(len(batch), device=DEVICE), actions]\n",
    "    loss_per = -(proj * torch.log(dist + 1e-8)).sum(1)\n",
    "    loss = (loss_per * weights).mean()\n",
    "    return loss, loss_per.detach().cpu().numpy(), dist_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b04e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Rainbow: Evaluation ----\n",
    "def evaluate_rainbow(\n",
    "    model: RainbowDQN,\n",
    "    episodes: int = 30,\n",
    "    seed: int = 123,\n",
    "    max_random_noops: int = 30,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"Evaluate Rainbow DQN agent with random no-op starts.\n",
    "    \n",
    "    Args:\n",
    "        model: The RainbowDQN model to evaluate.\n",
    "        episodes: Number of evaluation episodes.\n",
    "        seed: Random seed for reproducibility.\n",
    "        max_random_noops: Maximum number of random no-op actions at episode start.\n",
    "        verbose: If True, print per-episode scores and summary statistics.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (mean_score, std_score).\n",
    "    \"\"\"\n",
    "    env = make_atari_env(seed=seed, episodic_life=False, sticky_actions=True)\n",
    "    noop_action = get_noop_action(env)\n",
    "    \n",
    "    model.eval()\n",
    "    model.set_noise(False)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    returns = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[RAINBOW EVAL] Starting evaluation: {episodes} episodes, max_noops={max_random_noops}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset(seed=seed + ep)\n",
    "        obs = fix_obs(obs).astype(np.uint8)\n",
    "        \n",
    "        # Random no-op actions at episode start\n",
    "        n_noops = int(rng.integers(0, max_random_noops + 1))\n",
    "        terminated = truncated = False\n",
    "        for _ in range(n_noops):\n",
    "            obs, _, terminated, truncated, _ = env.step(noop_action)\n",
    "            obs = fix_obs(obs).astype(np.uint8)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        total_reward = 0.0\n",
    "\n",
    "        while not done:\n",
    "            obs_t = torch.from_numpy(obs[None]).to(DEVICE).float()\n",
    "            with torch.no_grad():\n",
    "                dist = model(obs_t)\n",
    "                q = (dist * model.support).sum(2)\n",
    "                action = int(q.argmax(1).item())\n",
    "\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            obs = fix_obs(obs).astype(np.uint8)\n",
    "            done = bool(terminated or truncated)\n",
    "            total_reward += float(reward)\n",
    "\n",
    "        returns.append(total_reward)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[RAINBOW EVAL] Episode {ep + 1}/{episodes} | Score: {total_reward:.0f} | NoOps: {n_noops}\")\n",
    "\n",
    "    env.close()\n",
    "    model.set_noise(True)\n",
    "    model.train()\n",
    "    \n",
    "    mean_score = float(np.mean(returns))\n",
    "    std_score = float(np.std(returns))\n",
    "    \n",
    "    if verbose:\n",
    "        print()\n",
    "        print_eval_summary(\"RAINBOW\", returns)\n",
    "    \n",
    "    return mean_score, std_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31327ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Rainbow: Training ----\n",
    "def train_rainbow(\n",
    "    *,\n",
    "    total_steps: int = TOTAL_STEPS,\n",
    "    num_envs: int = NUM_ENVS_RB,\n",
    "    eval_interval_steps: int = 1_000_000 // FRAMES_PER_STEP,\n",
    "    log_interval: int = 10_000,\n",
    "):\n",
    "    env = gym.vector.SyncVectorEnv(\n",
    "        [lambda i=i: make_atari_env(seed=i, episodic_life=True, sticky_actions=True) for i in range(num_envs)]\n",
    "    )\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    obs = np.stack([fix_obs(o) for o in obs]).astype(np.uint8)\n",
    "    action_dim = env.single_action_space.n\n",
    "\n",
    "    model = RainbowDQN(action_dim).to(DEVICE)\n",
    "    target = RainbowDQN(action_dim).to(DEVICE)\n",
    "    target.load_state_dict(model.state_dict())\n",
    "\n",
    "    opt = optim.Adam(model.parameters(), lr=LEARNING_RATE_RB, eps=ADAM_EPS)\n",
    "    buffer = PrioritizedReplayBuffer(size=REPLAY_SIZE, alpha=PER_ALPHA)\n",
    "    nbufs = [NStepBuffer(n=N_STEP, gamma=GAMMA) for _ in range(num_envs)]\n",
    "\n",
    "    rewards100 = deque(maxlen=100)\n",
    "    episode_rewards = np.zeros(num_envs, dtype=np.float32)\n",
    "    episode_count = 0\n",
    "\n",
    "    beta = PER_BETA_START\n",
    "    beta_increment = (PER_BETA_END - PER_BETA_START) / PER_BETA_ANNEAL_STEPS\n",
    "\n",
    "    last_loss = 0.0\n",
    "    avg100 = 0.0\n",
    "    q_mean = 0.0\n",
    "    q_max = 0.0\n",
    "    last_eval_mean = 0.0\n",
    "    last_eval_std = 0.0\n",
    "    best_avg = -float(\"inf\")\n",
    "\n",
    "    csv_file = open(csv_path, \"a\", newline=\"\")\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    for step in range(1, total_steps + 1):\n",
    "        # Action selection\n",
    "        model.reset_noise()\n",
    "        with torch.no_grad():\n",
    "            obs_t = torch.from_numpy(obs).to(DEVICE).float()\n",
    "            dist = model(obs_t)\n",
    "            q = (dist * model.support).sum(2)\n",
    "            actions = q.argmax(1).cpu().numpy()\n",
    "\n",
    "        # Env step\n",
    "        next_obs, rewards, terminated, truncated, _ = env.step(actions)\n",
    "        dones = np.logical_or(terminated, truncated)\n",
    "        next_obs = np.stack([fix_obs(o) for o in next_obs]).astype(np.uint8)\n",
    "\n",
    "        # Store transitions / episode tracking\n",
    "        for i in range(num_envs):\n",
    "            r_clip = float(np.clip(rewards[i], -1.0, 1.0))\n",
    "            done = bool(dones[i])\n",
    "\n",
    "            nbufs[i].add(obs[i], int(actions[i]), r_clip, next_obs[i], done)\n",
    "            episode_rewards[i] += float(rewards[i])\n",
    "\n",
    "            out = nbufs[i].pop_nstep_if_ready()\n",
    "            if out is not None:\n",
    "                buffer.add(out)\n",
    "\n",
    "            if done:\n",
    "                for t in nbufs[i].flush_all():\n",
    "                    buffer.add(t)\n",
    "\n",
    "                rewards100.append(float(episode_rewards[i]))\n",
    "                episode_rewards[i] = 0.0\n",
    "                episode_count += 1\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        # Train update\n",
    "        if step > LEARNING_START and buffer.tree.n_entries >= BATCH_SIZE:\n",
    "            beta = min(PER_BETA_END, beta + beta_increment)\n",
    "            batch, tree_idxs, weights = buffer.sample(BATCH_SIZE, beta)\n",
    "\n",
    "            if batch is not None:\n",
    "                loss, per_sample_loss, dist_all = rainbow_loss(model, target, batch, weights)\n",
    "                last_loss = float(loss.item())\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "                opt.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    q_vals = (dist_all * model.support).sum(2)\n",
    "                    q_mean = float(q_vals.mean().item())\n",
    "                    q_max = float(q_vals.max().item())\n",
    "\n",
    "                buffer.update(tree_idxs, per_sample_loss + 1e-6)\n",
    "\n",
    "        # Target update\n",
    "        if step % TARGET_UPDATE == 0:\n",
    "            target.load_state_dict(model.state_dict())\n",
    "\n",
    "        # Eval\n",
    "        if step % eval_interval_steps == 0:\n",
    "            last_eval_mean, last_eval_std = evaluate_rainbow(model, episodes=30, seed=123)\n",
    "            print(f\"[RNBW][EVAL] Step {step} | Mean {last_eval_mean:.1f} | Std {last_eval_std:.1f}\")\n",
    "\n",
    "        # Logging\n",
    "        if step % log_interval == 0:\n",
    "            avg100 = float(np.mean(rewards100)) if rewards100 else 0.0\n",
    "            if episode_count > 0:\n",
    "                print(f\"[RNBW] Step {step}/{total_steps} | Ep {episode_count} | Avg100 {avg100:.1f} | Loss {last_loss:.4f}\")\n",
    "\n",
    "            csv_writer.writerow([\n",
    "                step,\n",
    "                rewards100[-1] if rewards100 else 0.0,\n",
    "                avg100,\n",
    "                last_loss,\n",
    "                q_mean,\n",
    "                q_max,\n",
    "                last_eval_mean,\n",
    "                last_eval_std,\n",
    "            ])\n",
    "            csv_file.flush()\n",
    "\n",
    "            # Best model checkpointing\n",
    "            if avg100 > best_avg and len(rewards100) >= 10:\n",
    "                best_avg = avg100\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"step\": step,\n",
    "                        \"model_state_dict\": model.state_dict(),\n",
    "                        \"target_state_dict\": target.state_dict(),\n",
    "                        \"avg_reward_100\": avg100,\n",
    "                    },\n",
    "                    os.path.join(SAVE_DIR, \"rainbow_best.pth\"),\n",
    "                )\n",
    "\n",
    "        # Periodic checkpoint\n",
    "        if step % 1_000_000 == 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"step\": step,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"target_state_dict\": target.state_dict(),\n",
    "                    \"optimizer_state_dict\": opt.state_dict(),\n",
    "                },\n",
    "                os.path.join(SAVE_DIR, f\"rainbow_step_{step}.pth\"),\n",
    "            )\n",
    "            print(f\"[RNBW] Saved checkpoint: rainbow_step_{step}.pth\")\n",
    "\n",
    "    csv_file.close()\n",
    "    env.close()\n",
    "    print(\"[RNBW] Training done\")\n",
    "    return os.path.join(SAVE_DIR, \"rainbow_best.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9212e22a",
   "metadata": {},
   "source": [
    "## Running code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_DEFAULT_CONFIG = {\n",
    "    \"num_envs\": 8,\n",
    "    \"num_steps\": 512,\n",
    "    \"total_steps\": 100_000_000,\n",
    "    \"learning_rate\": 2.5e-4,\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"clip_epsilon\": 0.1,\n",
    "    \"epochs\": 4,\n",
    "    \"minibatch_size\": 512,\n",
    "    \"entropy_coef\": 0.01,\n",
    "    \"value_coef\": 0.5,\n",
    "    \"linear_lr_decay\": True,\n",
    "    \"checkpoint_path\": \"space_invaders_final_ppo.pth\",\n",
    "    \"save_every\": 1_000_000,\n",
    "    \"save_template\": \"space_invaders_ppo_{}M.pth\",\n",
    "    \"load_path\": None,\n",
    "}\n",
    "\n",
    "# Space Invaders action space size (NOOP, FIRE, RIGHT, LEFT, RIGHTFIRE, LEFTFIRE)\n",
    "SPACE_INVADERS_ACTION_DIM = 6\n",
    "\n",
    "def compare_agents(\n",
    "    *,\n",
    "    ppo_ckpt: str = \"./pretrained_models/space_invaders_ppo_124M.pth\",\n",
    "    rainbow_ckpt: str = \"./pretrained_models/space_invaders_rainbow_best.pth\",\n",
    "    episodes: int = 100,\n",
    "    max_random_noops: int = 30,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"Compare PPO and Rainbow agents.\n",
    "    \n",
    "    Args:\n",
    "        ppo_ckpt: Path to PPO checkpoint.\n",
    "        rainbow_ckpt: Path to Rainbow checkpoint.\n",
    "        episodes: Number of evaluation episodes per agent.\n",
    "        max_random_noops: Maximum random no-op actions at episode start.\n",
    "        verbose: If True, print per-episode scores and summary statistics.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results for each agent: {\"PPO\": (mean, std), \"Rainbow\": (mean, std)}.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    if os.path.exists(ppo_ckpt):\n",
    "        m, s = evaluate_ppo(\n",
    "            ppo_ckpt,\n",
    "            episodes=episodes,\n",
    "            max_random_noops=max_random_noops,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        results[\"PPO\"] = (m, s)\n",
    "\n",
    "    else:\n",
    "        results[\"PPO\"] = None\n",
    "\n",
    "    if rainbow_ckpt is None:\n",
    "        rainbow_ckpt = os.path.join(SAVE_DIR, \"rainbow_best.pth\")\n",
    "\n",
    "    if os.path.exists(rainbow_ckpt):\n",
    "        chk = torch.load(rainbow_ckpt, map_location=DEVICE)\n",
    "        model = RainbowDQN(SPACE_INVADERS_ACTION_DIM).to(DEVICE)\n",
    "        model.load_state_dict(chk[\"model_state_dict\"])\n",
    "        m, s = evaluate_rainbow(\n",
    "            model,\n",
    "            episodes=episodes,\n",
    "            max_random_noops=max_random_noops,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        results[\"Rainbow\"] = (m, s)\n",
    "        \n",
    "    else:\n",
    "        results[\"Rainbow\"] = None\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b912bb31",
   "metadata": {},
   "source": [
    "## Train PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5540efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained models can be found in the pretrained_models folder\n",
    "\n",
    "#ppo_trained = train_ppo(PPO_DEFAULT_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acccbe76",
   "metadata": {},
   "source": [
    "## Train Rainbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e5b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a long time to train fully, limited to 200k as PoC\n",
    "# Pretrained models can be found in the pretrained_models folder\n",
    "\n",
    "#rainbow_trained = train_rainbow(total_steps=200_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3de4ba",
   "metadata": {},
   "source": [
    "## Compare the two models\n",
    "\n",
    "Returns mean score and stddev for each agent over specified episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98201fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PPO\n",
    "ppo_mean, ppo_std = evaluate_ppo(\n",
    "    checkpoint_path=\"./pretrained_models/space_invaders_ppo_124M.pth\",\n",
    "    episodes=1000,\n",
    "    verbose=True,   # toggle logging here\n",
    ")\n",
    "\n",
    "print(\"PPO:\", ppo_mean, \"+/-\", ppo_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c26cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Rainbow\n",
    "# load pretrained checkpoint\n",
    "rainbow_ckpt = \"./pretrained_models/space_invaders_rainbow_best.pth\"\n",
    "chk = torch.load(rainbow_ckpt, map_location=DEVICE)\n",
    "\n",
    "rainbow_model = RainbowDQN(action_dim=6).to(DEVICE)\n",
    "rainbow_model.load_state_dict(chk[\"model_state_dict\"])\n",
    "\n",
    "rainbow_mean, rainbow_std = evaluate_rainbow(\n",
    "    model=rainbow_model,\n",
    "    episodes=1000,\n",
    "    verbose=True,   # toggle logging here\n",
    ")\n",
    "\n",
    "print(\"Rainbow:\", rainbow_mean, \"+/-\", rainbow_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare_agents(episodes=100)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
